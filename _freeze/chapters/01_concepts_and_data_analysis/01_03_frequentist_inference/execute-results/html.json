{
  "hash": "f46e6a9344434dc4dd0c270d5f3cbb09",
  "result": {
    "engine": "jupyter",
    "markdown": "# Frequentist inference\n\nUnder the frequentist paradigm, we view the data as a random sample from a larger, potentially hypothetical populations.\n\n## Confidence intervals\nLet us assume that we flip a coin 100 times, and we observe 44 heads and 56 tails. We can view these 100 flips as a random sample from a much larger infinite hypothetical population of flips from this coin. In this case we can say that each flip, $X_{i}$, follows a Bournelli distribution with some probability $p$, i.e. $X_{i} \\sim \\mbox{Bernoulli}(p)$.\n\nWhat is our best estimate of the probability of getting a head, or what is our best estimate of $p$? We can also ask how confident are we in that estimate. We can start down the mathematical approach by applying the Central Limit Theorem (CLT):\n$$\n\\sum_{i}^{100} X_{i} \\underset{\\cdot}{\\overset{\\cdot}{\\sim}} \\mbox{N}(100p, 100p(1-p))\n$$\n\nSince we have observed 44 heads, we estimate $\\hat{p} = 44/100 = 0.44$. We use this value to construct a 95% confidence interval (CI):\n$$\n\\begin{align}\n(100p - 1.96 \\sqrt{100 p (1 - p)}&, 100p + 1.96 \\sqrt{100 p (1 - p)}) \\\\\n(44 - 1.96 \\sqrt{44 (0.56)}&, 44 + 1.96 \\sqrt{44 (0.56)}) \\\\\n(34.3&, 53.7)\n\\end{align}\n$$\n\nWe're 95% confident that the true probability of getting a head is in this interval. If we ask ourselves whether we think this is a fair coin, then it is reasonable that this is a fair coin because 50 is in this interval. \n\n::: {#cell-fig-confidence-intervals-100-coin-flips .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\n\nn_trials = 100\nn_flips = 100\ntrue_p = 0.5\n\np_hat = np.zeros((n_trials,))\nfor i in range(n_trials):\n    xs = scipy.stats.bernoulli(p=true_p).rvs(size=n_flips)\n    p_hat[i] = np.mean(xs)\nsd = [\n    scipy.stats.norm.ppf(0.975) * np.sqrt(n_flips * p_ * (1 - p_)) / n_flips\n    for p_ in p_hat\n]\nlower = p_hat - sd\nupper = p_hat + sd\nmiss = (upper < true_p) | (lower > true_p)\n\nfig, ax = plt.subplots()\nfor i in range(n_trials):\n    color = \"orange\" if miss[i] else \"lightgray\"\n    ax.errorbar(\n        i + 1, \n        p_hat[i], \n        yerr=sd[i],\n        ls=\"none\",\n        marker=\"o\",\n        c=color\n    )\nax.axhline(true_p, ls=\"-\", c=\"r\")\nax.set_xlabel(\"trial\")\nax.set_ylabel(\"$\\hat{p}$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The 95% confidence interval for 100 coin flips. We expect that the true probability of heads, depicted by the red line at 0.5, is enclosed in about 1 in 20 trials.](01_03_frequentist_inference_files/figure-html/fig-confidence-intervals-100-coin-flips-output-1.png){#fig-confidence-intervals-100-coin-flips width=661 height=470}\n:::\n:::\n\n\n## Likelihood function and maximum likelihood\n\nLet's do another example. Consider a hospital where 400 patients are admitted over a month for heart attacks, and a month later 72 of them have died and 328 of them have survived. We can ask, what's our estimate of the mortality rate? Under the frequentist paradigm, we must first establish our reference population. What do we think our reference population is here? One possibility is we could think about heart attack patients in the region. Another is we could think about heart attack patients that are admitted to this hospital, but over a longer period of time. \n\nHow might we do some estimation? We can say each patient comes from a Bernoulli distribution with unknown parameter $\\theta$: $Y_{i} \\sim \\mbox{Bernoulli}(\\theta)$, which means that $\\mbox{Pr}(Y_{i}=1) = \\theta$, where 1 encodes for having died.\n\nThe probability density function for the entire dataset can be written in vector form:\n$$\n\\mbox{Pr}(\\mathbf{Y} = \\mathbf{y} \\mid \\theta) = \\mbox{Pr}(Y_{1} = y_{1}, Y_{2} = y_{2}, \\ldots, Y_{N} = y_{N} \\mid \\theta)\n$$\n\nIf we assume each is independent, we can write:\n$$\n\\begin{align}\n\\mbox{Pr}(Y_{1} = y_{1}, Y_{2} = y_{2}, \\ldots, Y_{N} = y_{N} \\mid \\theta) &= \\mbox{Pr}(Y_{1} = y_{1} \\mid \\theta)\\ \\mbox{Pr}(Y_{2} = y_{2} \\mid \\theta)\\ \\ldots\\ \\mbox{Pr}(Y_{N} = y_{N} \\mid \\theta) \\\\\n&= \\prod_{i=1}^{N} \\mbox{Pr}(Y_{i} = y_{i} \\mid \\theta) \\\\\n&= \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\\\\n\\end{align}\n$$\n\nWe can think of the latter expression as a function of $\\theta$. This is the concept of a likelihood, the density function thought of as a function of the parameters:\n$$\nL(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}}\n$$\n\nOne way to estimate $\\theta$ is to choose the value that gives the largest value of the likelihood. This is referred to as the maximum likelihood estimate (MLE). \n$$\n\\hat{\\theta}_{\\mathrm{mle}} = \\arg \\max_{\\theta} L(\\theta \\mid \\mathbf{y})\n$$\n\nIn practice it is often easier to maximize the natural logarithm of the likekihood, referred to as the log-likelihood:\n$$\n\\ell(\\theta) = \\log L(\\theta \\mid \\mathbf{y})\n$$\n\nIn this case:\n$$\n\\begin{align}\n\\ell(\\theta) &= \\log\\left( \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\right) \\\\\n &= \\sum_{i=1}^{N} \\log \\left(\\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\right) \\\\\n &= \\sum_{i=1}^{N} y_{i} \\log(\\theta) + (1 - y_{i}) \\log(1 - \\theta) \\\\\n &= \\left(\\sum_{i=1}^{N} y_{i}\\right) \\log(\\theta) + \\left(\\sum_{i=1}^{N} (1 - y_{i})\\right) \\log(1 - \\theta) \\\\\n\\end{align}\n$$\n\n## Computing the MLE\nWe find the maximum log-likelihood by taking the derivative and setting it to zero:\n$$\n\\begin{align}\n\\ell'(\\theta) &= \\frac{1}{\\theta} \\left(\\sum_{i=1}^{N} y_{i}\\right) - \\frac{1}{1 - \\theta} \\left(\\sum_{i=1}^{N} (1 - y_{i})\\right) \\overset{\\text{set}}{=} 0 \\\\ \n&= \\frac{\\sum_{i=1}^{N} y_{i}}{\\hat{\\theta}} = \\frac{\\sum_{i=1}^{N}(1 - y_{i})}{1 - \\hat{\\theta}} \\\\\n&\\Rightarrow \\hat{\\theta} = \\frac{\\sum_{i=1}^{N}y_{i}}{N}\n\\end{align}\n$$\n\n::: {#d65a73e1 .cell execution_count=2}\n``` {.python .cell-code}\np_mortality = 72 / 400.\n```\n:::\n\n\nThis results in $\\hat{p}$ = 0.18 as the MLE of the probability.\n\n",
    "supporting": [
      "01_03_frequentist_inference_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}