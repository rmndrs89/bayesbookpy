{
  "hash": "71594db5d58632412b6775027556833f",
  "result": {
    "engine": "jupyter",
    "markdown": "# Tossing a coin\n\n## Frequentist inference\nSuppose your brother has a coin which you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you're not sure which one and he wants to make a bet with you that it's going to come up heads. You're not sure of it's the loaded coin or if it's just a fair one. \n\nTherefore he gives you a chance to flip it five times and just check it out. You flip it five times and you get two heads and three tails. Now you have to make a decision. Which coin do you think it is and how sure are you about that? \n\nWe start by defining the unknown parameter $\\theta$, that is either the coin is fair or it's loaded.\n\nThe data likelihood follow a binomial distribution, so we can write:\n$$\n\\begin{align}\n\\theta &\\in \\left\\{\\mathrm{fair}, \\mathrm{loaded} \\right\\} \\\\\nX &\\sim \\mbox{Bin}(5, p) \\\\\n\\Rightarrow p(x \\mid \\theta) &= \\begin{cases}\n \\binom{5}{x} \\left(\\frac{1}{2}\\right)^{5} & \\text{if }\\theta \\text{ is fair} \\\\\n \\binom{5}{x} (0.7)^{x} (0.3)^{5-x} & \\text{if }\\theta \\text{ is loaded} \\\\\n \\end{cases} \\\\\n &= \\binom{5}{x} \\left(\\frac{1}{2}\\right)^{5} \\mathbb{I}_{\\left\\{\\theta=\\text{fair}\\right\\}} + \\binom{5}{x} (0.7)^{x} (0.3)^{5-x} \\mathbb{I}_{\\left\\{\\theta=\\text{loaded}\\right\\}}\n\\end{align}\n$$\n\nIn this case we have observed $x = 2$, so what is the likelihood:\n$$\np(\\theta \\mid X=2) = \\begin{cases}\n \\binom{5}{2} \\left(\\frac{1}{2}\\right)^{5} & \\text{if }\\theta \\text{ is fair} \\\\\n \\binom{5}{2} (0.7)^{2} (0.3)^{5-2} & \\text{if }\\theta \\text{ is loaded} \\\\\n\\end{cases}\n$$\n\n::: {#cell-fig-likelihood-two-heads .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport seaborn as sns\n\nlls = [\n    scipy.stats.binom.pmf(k=2, n=5, p=0.5),\n    scipy.stats.binom.pmf(k=2, n=5, p=0.7)\n]\n\nfig, ax = plt.subplots(figsize=(2, 2))\nsns.barplot(x=np.array([0, 1]), y=lls)\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nax.set_xticklabels([\"fair\", \"loaded\"])\nax.set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\nax.set_ylim((0, 0.5))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_43757/2665880107.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"fair\", \"loaded\"])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![The likelihood of observing two heads in five coin flips for the fair and loaded coin.](01_04_tossing_a_coin_files/figure-html/fig-likelihood-two-heads-output-2.png){#fig-likelihood-two-heads}\n:::\n:::\n\n\nThis point estimate is great, but how sure are we? That is not an easy question to answer in the frequentist paradigm. We also might like to know the probability that the coin is fair, given that we observed two heads. In the frequentist paradigm, the coin is a physical quantity. It's a fixed coin, and therefore it has a fixed probability of coming up heads. That probability is either 0 or 1. \n\n## Bayesian inference\nWith the Bayesian approach, you can easily incorporate previous knowledge. For example, since we know our brother pretty well, we have a prior belief of 60% that he's brought the loaded coin. We can then used Bayes' theorem to compute the posterior probability:\n$$\n\\begin{align}\np(\\theta \\mid x) &= p(x \\mid \\theta)\\ p(\\theta) / \\sum_{\\theta}p(x \\mid \\theta)\\ p(\\theta) \\\\\n &= \\frac{\\binom{5}{x} \\left[ \\left(\\frac{1}{2}\\right)^{5} (0.4) \\mathbb{I}_{\\left\\{\\theta=\\mathrm{fair} \\right\\}} + \\left(0.7\\right)^{x} \\left(0.3\\right)^{5-x} (0.6) \\mathbb{I}_{\\left\\{\\theta=\\mathrm{loaded} \\right\\}} \\right]}{\\binom{5}{x} \\left[ \\left(\\frac{1}{2}\\right)^{5} (0.4) + \\left(0.7\\right)^{x} \\left(0.3\\right)^{5-x} (0.6) \\right]}\n\\end{align}\n$$\n\n::: {#cb9083db .cell execution_count=2}\n``` {.python .cell-code}\nprior_ = np.array([0.4, 0.6])  # prior probability for theta\nprobs_ = np.array([0.5, 0.7])  # probability of heads for both cases\n\n# Compute the posterior probability\nposterior_ = np.asarray([\n    scipy.stats.binom.pmf(k=2, n=5, p=p) * p0\n    for p, p0 in zip(probs_, prior_)\n])\nposterior_ /= np.sum(posterior_)\n\nfig, ax = plt.subplots(figsize=(2, 2))\nsns.barplot(x=np.array([0, 1]), y=posterior_)\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nax.set_xticklabels([\"fair\", \"loaded\"])\nax.set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\nax.set_ylim((0, 0.8))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_43757/2217390571.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"fair\", \"loaded\"])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](01_04_tossing_a_coin_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\nSo our posterior probability that this is the loaded coin works out to be 0.388. Isn't that a much more satisfying answer? Under the Bayesian approach, we get a probability, and we can actually interpret this probability.\n\nWe can also examine what would happen under different choices of prior. We did this calculation with the prior probability of 0.6 for the coin being loaded, but we might have a different idea and want to use a different probability. We can use anything between zero and one. \n\n::: {#d0212150 .cell execution_count=3}\n``` {.python .cell-code}\n# prior_ = np.array([0.4, 0.6])  # prior probability for theta\nprobs_ = np.array([0.5, 0.7])  # probability of heads for both cases\n\nfig, axs = plt.subplots(1, 3, figsize=(6, 2), sharey=True)\nfor ax, x in zip(axs, [0.3, 0.6, 0.9]):\n    prior_ = np.array([1 - x, x])\n    posterior_ = np.asarray([\n        scipy.stats.binom.pmf(k=2, n=5, p=p) * p0\n        for p, p0 in zip(probs_, prior_)\n    ])\n    posterior_ /= np.sum(posterior_)\n\n    sns.barplot(x=np.array([0, 1]), y=posterior_, ax=ax)\n    for container in ax.containers:\n        ax.bar_label(container, fmt=\"%.3f\")\n    ax.set_title(f\"$p$($\\\\theta$)={x}\", fontsize=10)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"fair\", \"loaded\"])\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\naxs[0].set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\naxs[0].set_ylim((0, 1.0))\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01_04_tossing_a_coin_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n## Continuous\n\n::: {#3f502378 .cell execution_count=4}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(6, 3))\naxs[0].plot([0, 1], [1, 1], lw=2, c=\"tab:orange\")\naxs[0].plot([0, 0], [0, 1], lw=1, ls=\"--\", c=\"tab:orange\")\naxs[0].plot([1, 1], [0, 1], lw=1, ls=\"--\", c=\"tab:orange\")\naxs[1].plot([0, 1], [0, 2], lw=2, c=\"tab:orange\")\naxs[1].plot([1, 1], [0, 2], lw=1, ls=\"--\", c=\"tab:orange\")\nfor ax in axs:\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.set_xlabel(f\"$\\\\theta$\")\naxs[0].set_ylabel(f\"$p$($\\\\theta$)\")\naxs[0].set_title(\"prior\")\naxs[1].set_ylabel(f\"$p$($\\\\theta$ | $X$=1)\")\naxs[1].set_title(\"posterior\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01_04_tossing_a_coin_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nWe can look at prior and posterior interval estimates. Under the prior we have:\n$$\n\\begin{align}\n\\mbox{Pr}(0.025 \\le \\theta \\le 0.975) &= 0.95 \\\\\n\\mbox{Pr}(\\theta > 0.05) &= 0.95 \\\\\n\\end{align}\n$$\n\nUnder the posterior we have:\n$$\n\\begin{align}\n\\mbox{Pr}(0.025 \\le \\theta \\le 0.975) &= \\int_{0.025}^{0.975} 2 \\theta\\ \\mathrm{d}\\theta = (0.975)^{2} - (0.025)^{2} = 0.95\\\\\n\\mbox{Pr}(\\theta > 0.05) &= 1 - (0.05)^{2} = 0.9975 \\\\\n\\end{align}\n$$\n\nWe can also ask what is an interval that contains 95% of the posterior probability?\n\nThere are two main types of intervals that are of interest in the Bayesian paradigm:  \n1. equal-tailed intervals  \n2. highest posterior density intervals  \n\n### Equal-tailed intervals\nWe put an equal amount of probability in each tail. Thus, to make a 95% interval we put 0.025 in each tail.\n$$\n\\begin{align}\np(\\theta < q \\mid Y=1) &= \\int_{0}^{q} 2 \\theta\\ \\mathrm{d}\\theta = q^{2} \\\\\np(\\sqrt{0.025} \\le \\theta \\le \\sqrt{0.975}) &= 0.95\n\\end{align}\n$$\n\n### Highest-posterior density (HPD)\nWhere in the density is it hightest?\n$$\np(\\theta > \\sqrt{0.05} \\mid Y=1) = p(\\theta > 0.224 \\mid Y=1) = 0.95\n$$\n\n",
    "supporting": [
      "01_04_tossing_a_coin_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}