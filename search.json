[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Data Analysis with Python",
    "section": "",
    "text": "Preface\nThis book is about doing data analysis in Python using a Bayesian approach. The contents are mainly based on the Coursera specialization about Bayesian Statistics (Heiner et al. n.d.), but will gradually be supplemented with practical examples and additional theory from other resources such as Doing Bayesian Data Analysis (Kruschke 2015).\n\n\n\n\nHeiner, Matthew, Herbert Lee, Abel Rodriguez, Raquel Prado, and Jizhou Kang. n.d. “Bayesian Statistics.” Coursera.\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. 2nd ed. Academic Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_01_intro.html",
    "href": "chapters/01_concepts_and_data_analysis/01_01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Kruschke (2015) for additional discussion of literate programming.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\n\nxs = np.linspace(-5, 5, num=500, endpoint=False)\nys = scipy.stats.norm.pdf(x=xs, loc=0.0, scale=1)\nfig, ax = plt.subplots(figsize=(2.8, 2.8))\nax.plot(xs, ys, lw=2, c=\"gray\")\nax.set_xlabel(f\"$x$\")\nax.set_ylabel(f\"$p(x)$\")\nax.set_aspect(\"auto\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.1: A standard normal distribution.\n\n\n\n\n\n\n\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. 2nd ed. Academic Press.",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_02_basics_of_probability.html",
    "href": "chapters/01_concepts_and_data_analysis/01_02_basics_of_probability.html",
    "title": "2  Basics of probability",
    "section": "",
    "text": "2.1 Random variables\nStatistics is the study of uncertainty. How do we measure uncertainty? Typically we deal with uncertainty by means of probabilities (Kruschke 2015). But what are then probabilities?\nThere are roughly three different frameworks to define probabilities: the classical framework, the frequentist framework, and the Bayesian framework.\nIn general, a probability, under any of these frameworks, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called “probabilities”, just need to satisfy three properties (Kolmogorov 2018):\nUnder the classical framework, outcomes that are equally likely have equal probabilities. For example, when rolling a fair die, there are six possible outcomes, and they’re all equally likely, thus the probability of rolling a four is just \\(\\mbox{Pr}(X = 4) = \\frac{1}{6}\\).\nUnder the frequentist framework, probabilities are regarded to as long-run relative frequencies.\nSuppose that \\(X\\) represents some unknown quantity of interest. If the value of \\(X\\) is unknown, or could change, we call it a random variable. The set of possible values, denoted \\(\\mathcal{X}\\), is called the sample space. An event is a set of outcomes from a given sample space (Murphy 2022).",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of probability</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_02_basics_of_probability.html#random-variables",
    "href": "chapters/01_concepts_and_data_analysis/01_02_basics_of_probability.html#random-variables",
    "title": "2  Basics of probability",
    "section": "",
    "text": "TipExample: random variables\n\n\n\nIf \\(X\\) represents the face of a dice that is rolled, i.e., \\(\\mathcal{X} = \\left\\{1, 2, \\ldots, 6 \\right\\}\\), the event of “seeing a 1” is denoted \\(X = 1\\), the event of “seeing an odd number” is \\(X \\in \\left\\{1, 3, 5 \\right\\}\\).\n\n\n\n2.1.1 Events\nThe expression \\(\\mbox{Pr}(A)\\) is used to denote the probability with which you believe an event \\(A\\) is true. If an event will definitely happen, we write \\(\\mbox{Pr}(A)=1\\) and if an event is definitely not happening we write \\(\\mbox{Pr}(A)=0\\). Furthermore, the probability of \\(A\\) not happening is denoted \\(\\mbox{Pr}(\\bar{A}) = 1 - \\mbox{Pr}(A)\\).\n\n\n\n\nKolmogorov, Andreĭ Nikolaevich. 2018. Foundations of the Theory of Probability: Second English Edition. 2nd ed. Dover Publications.\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. 2nd ed. Academic Press.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of probability</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html",
    "href": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html",
    "title": "3  Frequentist inference",
    "section": "",
    "text": "3.1 Confidence intervals\nUnder the frequentist paradigm, we view the data as a random sample from a larger, potentially hypothetical populations.\nLet us assume that we flip a coin 100 times, and we observe 44 heads and 56 tails. We can view these 100 flips as a random sample from a much larger infinite hypothetical population of flips from this coin. In this case we can say that each flip, \\(X_{i}\\), follows a Bournelli distribution with some probability \\(p\\), i.e. \\(X_{i} \\sim \\mbox{Bernoulli}(p)\\).\nWhat is our best estimate of the probability of getting a head, or what is our best estimate of \\(p\\)? We can also ask how confident are we in that estimate. We can start down the mathematical approach by applying the Central Limit Theorem (CLT): \\[\n\\sum_{i}^{100} X_{i} \\underset{\\cdot}{\\overset{\\cdot}{\\sim}} \\mbox{N}(100p, 100p(1-p))\n\\]\nSince we have observed 44 heads, we estimate \\(\\hat{p} = 44/100 = 0.44\\). We use this value to construct a 95% confidence interval (CI): \\[\n\\begin{align}\n(100p - 1.96 \\sqrt{100 p (1 - p)}&, 100p + 1.96 \\sqrt{100 p (1 - p)}) \\\\\n(44 - 1.96 \\sqrt{44 (0.56)}&, 44 + 1.96 \\sqrt{44 (0.56)}) \\\\\n(34.3&, 53.7)\n\\end{align}\n\\]\nWe’re 95% confident that the true probability of getting a head is in this interval. If we ask ourselves whether we think this is a fair coin, then it is reasonable that this is a fair coin because 50 is in this interval.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\n\nn_trials = 100\nn_flips = 100\ntrue_p = 0.5\n\np_hat = np.zeros((n_trials,))\nfor i in range(n_trials):\n    xs = scipy.stats.bernoulli(p=true_p).rvs(size=n_flips)\n    p_hat[i] = np.mean(xs)\nsd = [\n    scipy.stats.norm.ppf(0.975) * np.sqrt(n_flips * p_ * (1 - p_)) / n_flips\n    for p_ in p_hat\n]\nlower = p_hat - sd\nupper = p_hat + sd\nmiss = (upper &lt; true_p) | (lower &gt; true_p)\n\nfig, ax = plt.subplots()\nfor i in range(n_trials):\n    color = \"orange\" if miss[i] else \"lightgray\"\n    ax.errorbar(\n        i + 1, \n        p_hat[i], \n        yerr=sd[i],\n        ls=\"none\",\n        marker=\"o\",\n        c=color\n    )\nax.axhline(true_p, ls=\"-\", c=\"r\")\nax.set_xlabel(\"trial\")\nax.set_ylabel(\"$\\hat{p}$\")\nplt.tight_layout()\nplt.show()\n\n\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\h'\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\h'\n/tmp/ipykernel_21084/3621781736.py:34: SyntaxWarning: invalid escape sequence '\\h'\n  ax.set_ylabel(\"$\\hat{p}$\")\n\n\n\n\n\n\n\n\nFigure 3.1: The 95% confidence interval for 100 coin flips. We expect that the true probability of heads, depicted by the red line at 0.5, is enclosed in about 1 in 20 trials.",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Frequentist inference</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#likelihood-function-and-maximum-likelihood",
    "href": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#likelihood-function-and-maximum-likelihood",
    "title": "3  Frequentist inference",
    "section": "3.2 Likelihood function and maximum likelihood",
    "text": "3.2 Likelihood function and maximum likelihood\nLet’s do another example. Consider a hospital where 400 patients are admitted over a month for heart attacks, and a month later 72 of them have died and 328 of them have survived. We can ask, what’s our estimate of the mortality rate? Under the frequentist paradigm, we must first establish our reference population. What do we think our reference population is here? One possibility is we could think about heart attack patients in the region. Another is we could think about heart attack patients that are admitted to this hospital, but over a longer period of time.\nHow might we do some estimation? We can say each patient comes from a Bernoulli distribution with unknown parameter \\(\\theta\\): \\(Y_{i} \\sim \\mbox{Bernoulli}(\\theta)\\), which means that \\(\\mbox{Pr}(Y_{i}=1) = \\theta\\), where 1 encodes for having died.\nThe probability density function for the entire dataset can be written in vector form: \\[\n\\mbox{Pr}(\\mathbf{Y} = \\mathbf{y} \\mid \\theta) = \\mbox{Pr}(Y_{1} = y_{1}, Y_{2} = y_{2}, \\ldots, Y_{N} = y_{N} \\mid \\theta)\n\\]\nIf we assume each is independent, we can write: \\[\n\\begin{align}\n\\mbox{Pr}(Y_{1} = y_{1}, Y_{2} = y_{2}, \\ldots, Y_{N} = y_{N} \\mid \\theta) &= \\mbox{Pr}(Y_{1} = y_{1} \\mid \\theta)\\ \\mbox{Pr}(Y_{2} = y_{2} \\mid \\theta)\\ \\ldots\\ \\mbox{Pr}(Y_{N} = y_{N} \\mid \\theta) \\\\\n&= \\prod_{i=1}^{N} \\mbox{Pr}(Y_{i} = y_{i} \\mid \\theta) \\\\\n&= \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\\\\n\\end{align}\n\\]\nWe can think of the latter expression as a function of \\(\\theta\\). This is the concept of a likelihood, the density function thought of as a function of the parameters: \\[\nL(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}}\n\\]\nOne way to estimate \\(\\theta\\) is to choose the value that gives the largest value of the likelihood. This is referred to as the maximum likelihood estimate (MLE). \\[\n\\hat{\\theta}_{\\mathrm{mle}} = \\arg \\max_{\\theta} L(\\theta \\mid \\mathbf{y})\n\\]\nIn practice it is often easier to maximize the natural logarithm of the likekihood, referred to as the log-likelihood: \\[\n\\ell(\\theta) = \\log L(\\theta \\mid \\mathbf{y})\n\\]\nIn this case: \\[\n\\begin{align}\n\\ell(\\theta) &= \\log\\left( \\prod_{i=1}^{N} \\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\right) \\\\\n&= \\sum_{i=1}^{N} \\log \\left(\\theta^{y_{i}}\\ (1 - \\theta)^{1 - y_{i}} \\right) \\\\\n&= \\sum_{i=1}^{N} y_{i} \\log(\\theta) + (1 - y_{i}) \\log(1 - \\theta) \\\\\n&= \\left(\\sum_{i=1}^{N} y_{i}\\right) \\log(\\theta) + \\left(\\sum_{i=1}^{N} (1 - y_{i})\\right) \\log(1 - \\theta) \\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Frequentist inference</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#computing-the-mle",
    "href": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#computing-the-mle",
    "title": "3  Frequentist inference",
    "section": "3.3 Computing the MLE",
    "text": "3.3 Computing the MLE\nWe find the maximum log-likelihood by taking the derivative and setting it to zero: \\[\n\\begin{align}\n\\ell'(\\theta) &= \\frac{1}{\\theta} \\left(\\sum_{i=1}^{N} y_{i}\\right) - \\frac{1}{1 - \\theta} \\left(\\sum_{i=1}^{N} (1 - y_{i})\\right) \\overset{\\text{set}}{=} 0 \\\\\n&= \\frac{\\sum_{i=1}^{N} y_{i}}{\\hat{\\theta}} = \\frac{\\sum_{i=1}^{N}(1 - y_{i})}{1 - \\hat{\\theta}} \\\\\n&\\Rightarrow \\hat{\\theta} = \\frac{\\sum_{i=1}^{N}y_{i}}{N}\n\\end{align}\n\\]\n\n\nCode\np_mortality = 72 / 400.\n\n\nThis results in \\(\\hat{p}\\) = 0.18 as the MLE of the probability.",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Frequentist inference</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#plotting-the-likelihoood",
    "href": "chapters/01_concepts_and_data_analysis/01_03_frequentist_inference.html#plotting-the-likelihoood",
    "title": "3  Frequentist inference",
    "section": "3.4 Plotting the likelihoood",
    "text": "3.4 Plotting the likelihoood\n\n\nCode\ndef likelihood(n: int, y: int, theta: float | np.ndarray[float]) -&gt; float | np.ndarray[float]:\n    return theta**y + (1 - theta)**(n - y)\n\ndef loglike(n, y, theta):\n    return y*np.log(theta) + (n-y)*np.log(1-theta)\n\nthetas = np.linspace(0.01, 0.99, num=99, endpoint=True)\nfig, ax = plt.subplots()\nax.plot(thetas, loglike(n=400, y=72, theta=thetas))\nplt.show()",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Frequentist inference</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html",
    "href": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html",
    "title": "4  Tossing a coin",
    "section": "",
    "text": "4.1 Frequentist inference\nSuppose your brother has a coin which you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you’re not sure which one and he wants to make a bet with you that it’s going to come up heads. You’re not sure of it’s the loaded coin or if it’s just a fair one.\nTherefore he gives you a chance to flip it five times and just check it out. You flip it five times and you get two heads and three tails. Now you have to make a decision. Which coin do you think it is and how sure are you about that?\nWe start by defining the unknown parameter \\(\\theta\\), that is either the coin is fair or it’s loaded.\nThe data likelihood follow a binomial distribution, so we can write: \\[\n\\begin{align}\n\\theta &\\in \\left\\{\\mathrm{fair}, \\mathrm{loaded} \\right\\} \\\\\nX &\\sim \\mbox{Bin}(5, p) \\\\\n\\Rightarrow p(x \\mid \\theta) &= \\begin{cases}\n\\binom{5}{x} \\left(\\frac{1}{2}\\right)^{5} & \\text{if }\\theta \\text{ is fair} \\\\\n\\binom{5}{x} (0.7)^{x} (0.3)^{5-x} & \\text{if }\\theta \\text{ is loaded} \\\\\n\\end{cases} \\\\\n&= \\binom{5}{x} \\left(\\frac{1}{2}\\right)^{5} \\mathbb{I}_{\\left\\{\\theta=\\text{fair}\\right\\}} + \\binom{5}{x} (0.7)^{x} (0.3)^{5-x} \\mathbb{I}_{\\left\\{\\theta=\\text{loaded}\\right\\}}\n\\end{align}\n\\]\nIn this case we have observed \\(x = 2\\), so what is the likelihood: \\[\np(\\theta \\mid X=2) = \\begin{cases}\n\\binom{5}{2} \\left(\\frac{1}{2}\\right)^{5} & \\text{if }\\theta \\text{ is fair} \\\\\n\\binom{5}{2} (0.7)^{2} (0.3)^{5-2} & \\text{if }\\theta \\text{ is loaded} \\\\\n\\end{cases}\n\\]\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport seaborn as sns\n\nlls = [\n    scipy.stats.binom.pmf(k=2, n=5, p=0.5),\n    scipy.stats.binom.pmf(k=2, n=5, p=0.7)\n]\n\nfig, ax = plt.subplots(figsize=(2, 2))\nsns.barplot(x=np.array([0, 1]), y=lls)\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nax.set_xticklabels([\"fair\", \"loaded\"])\nax.set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\nax.set_ylim((0, 0.5))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\n/tmp/ipykernel_43757/2665880107.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"fair\", \"loaded\"])\n\n\n\n\n\n\n\n\nFigure 4.1: The likelihood of observing two heads in five coin flips for the fair and loaded coin.\nThis point estimate is great, but how sure are we? That is not an easy question to answer in the frequentist paradigm. We also might like to know the probability that the coin is fair, given that we observed two heads. In the frequentist paradigm, the coin is a physical quantity. It’s a fixed coin, and therefore it has a fixed probability of coming up heads. That probability is either 0 or 1.",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tossing a coin</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html#bayesian-inference",
    "href": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html#bayesian-inference",
    "title": "4  Tossing a coin",
    "section": "4.2 Bayesian inference",
    "text": "4.2 Bayesian inference\nWith the Bayesian approach, you can easily incorporate previous knowledge. For example, since we know our brother pretty well, we have a prior belief of 60% that he’s brought the loaded coin. We can then used Bayes’ theorem to compute the posterior probability: \\[\n\\begin{align}\np(\\theta \\mid x) &= p(x \\mid \\theta)\\ p(\\theta) / \\sum_{\\theta}p(x \\mid \\theta)\\ p(\\theta) \\\\\n&= \\frac{\\binom{5}{x} \\left[ \\left(\\frac{1}{2}\\right)^{5} (0.4) \\mathbb{I}_{\\left\\{\\theta=\\mathrm{fair} \\right\\}} + \\left(0.7\\right)^{x} \\left(0.3\\right)^{5-x} (0.6) \\mathbb{I}_{\\left\\{\\theta=\\mathrm{loaded} \\right\\}} \\right]}{\\binom{5}{x} \\left[ \\left(\\frac{1}{2}\\right)^{5} (0.4) + \\left(0.7\\right)^{x} \\left(0.3\\right)^{5-x} (0.6) \\right]}\n\\end{align}\n\\]\n\n\nCode\nprior_ = np.array([0.4, 0.6])  # prior probability for theta\nprobs_ = np.array([0.5, 0.7])  # probability of heads for both cases\n\n# Compute the posterior probability\nposterior_ = np.asarray([\n    scipy.stats.binom.pmf(k=2, n=5, p=p) * p0\n    for p, p0 in zip(probs_, prior_)\n])\nposterior_ /= np.sum(posterior_)\n\nfig, ax = plt.subplots(figsize=(2, 2))\nsns.barplot(x=np.array([0, 1]), y=posterior_)\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nax.set_xticklabels([\"fair\", \"loaded\"])\nax.set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\nax.set_ylim((0, 0.8))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\n/tmp/ipykernel_43757/2217390571.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"fair\", \"loaded\"])\n\n\n\n\n\n\n\n\n\nSo our posterior probability that this is the loaded coin works out to be 0.388. Isn’t that a much more satisfying answer? Under the Bayesian approach, we get a probability, and we can actually interpret this probability.\nWe can also examine what would happen under different choices of prior. We did this calculation with the prior probability of 0.6 for the coin being loaded, but we might have a different idea and want to use a different probability. We can use anything between zero and one.\n\n\nCode\n# prior_ = np.array([0.4, 0.6])  # prior probability for theta\nprobs_ = np.array([0.5, 0.7])  # probability of heads for both cases\n\nfig, axs = plt.subplots(1, 3, figsize=(6, 2), sharey=True)\nfor ax, x in zip(axs, [0.3, 0.6, 0.9]):\n    prior_ = np.array([1 - x, x])\n    posterior_ = np.asarray([\n        scipy.stats.binom.pmf(k=2, n=5, p=p) * p0\n        for p, p0 in zip(probs_, prior_)\n    ])\n    posterior_ /= np.sum(posterior_)\n\n    sns.barplot(x=np.array([0, 1]), y=posterior_, ax=ax)\n    for container in ax.containers:\n        ax.bar_label(container, fmt=\"%.3f\")\n    ax.set_title(f\"$p$($\\\\theta$)={x}\", fontsize=10)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"fair\", \"loaded\"])\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\naxs[0].set_ylabel(f\"$p$($\\\\theta$ | $X$=2)\")\naxs[0].set_ylim((0, 1.0))\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tossing a coin</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html#continuous",
    "href": "chapters/01_concepts_and_data_analysis/01_04_tossing_a_coin.html#continuous",
    "title": "4  Tossing a coin",
    "section": "4.3 Continuous",
    "text": "4.3 Continuous\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(6, 3))\naxs[0].plot([0, 1], [1, 1], lw=2, c=\"tab:orange\")\naxs[0].plot([0, 0], [0, 1], lw=1, ls=\"--\", c=\"tab:orange\")\naxs[0].plot([1, 1], [0, 1], lw=1, ls=\"--\", c=\"tab:orange\")\naxs[1].plot([0, 1], [0, 2], lw=2, c=\"tab:orange\")\naxs[1].plot([1, 1], [0, 2], lw=1, ls=\"--\", c=\"tab:orange\")\nfor ax in axs:\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.set_xlabel(f\"$\\\\theta$\")\naxs[0].set_ylabel(f\"$p$($\\\\theta$)\")\naxs[0].set_title(\"prior\")\naxs[1].set_ylabel(f\"$p$($\\\\theta$ | $X$=1)\")\naxs[1].set_title(\"posterior\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can look at prior and posterior interval estimates. Under the prior we have: \\[\n\\begin{align}\n\\mbox{Pr}(0.025 \\le \\theta \\le 0.975) &= 0.95 \\\\\n\\mbox{Pr}(\\theta &gt; 0.05) &= 0.95 \\\\\n\\end{align}\n\\]\nUnder the posterior we have: \\[\n\\begin{align}\n\\mbox{Pr}(0.025 \\le \\theta \\le 0.975) &= \\int_{0.025}^{0.975} 2 \\theta\\ \\mathrm{d}\\theta = (0.975)^{2} - (0.025)^{2} = 0.95\\\\\n\\mbox{Pr}(\\theta &gt; 0.05) &= 1 - (0.05)^{2} = 0.9975 \\\\\n\\end{align}\n\\]\nWe can also ask what is an interval that contains 95% of the posterior probability?\nThere are two main types of intervals that are of interest in the Bayesian paradigm:\n1. equal-tailed intervals\n2. highest posterior density intervals\n\n4.3.1 Equal-tailed intervals\nWe put an equal amount of probability in each tail. Thus, to make a 95% interval we put 0.025 in each tail. \\[\n\\begin{align}\np(\\theta &lt; q \\mid Y=1) &= \\int_{0}^{q} 2 \\theta\\ \\mathrm{d}\\theta = q^{2} \\\\\np(\\sqrt{0.025} \\le \\theta \\le \\sqrt{0.975}) &= 0.95\n\\end{align}\n\\]\n\n\n4.3.2 Highest-posterior density (HPD)\nWhere in the density is it hightest? \\[\np(\\theta &gt; \\sqrt{0.05} \\mid Y=1) = p(\\theta &gt; 0.224 \\mid Y=1) = 0.95\n\\]",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tossing a coin</span>"
    ]
  },
  {
    "objectID": "chapters/01_concepts_and_data_analysis/01_05_continuous_variables.html",
    "href": "chapters/01_concepts_and_data_analysis/01_05_continuous_variables.html",
    "title": "5  Examples: continuous variables",
    "section": "",
    "text": "5.1 Exponential data\nSuppose we are waiting for a bus that is supposed to come every ten minutes. We can model this waiting time with the exponential distribution: \\[\nY \\sim \\mbox{Exp}(\\lambda)\n\\] where \\(\\mathbb{E}[Y] = 1 / \\lambda\\) and \\(\\mathbb{V}[Y] = 1 / \\lambda^{2}\\).\nIt turns out that the Gamma distribution is conjugate to the exponential distribution: \\[\n\\lambda \\sim \\mbox{Gamma}(a, b)\n\\] with \\(\\mathbb{E}[\\lambda] = a / b\\) and \\(\\mathbb{V}[\\lambda]=a / b^{2}\\)\nThus if we expect the bus to come around every ten minutes, we need \\(a / b = 1 / 10\\) or \\(b = 10a\\).\nSuppose that we wait for 12 minutes and the bus arrives. How can we update our belief of \\(\\lambda\\), that is the expected waiting time? Using Bayes’ theorem we have: \\[\n\\begin{align}\np(\\lambda \\mid y) &\\propto p(y \\mid \\lambda)\\ p(\\lambda) \\\\\n&\\propto \\lambda e^{-\\lambda y} \\lambda^{a-1} e^{-b \\lambda} \\\\\n&\\propto \\lambda^{(a+1)-1}\\ e^{-(b + \\lambda) y} \\\\\n\\lambda \\mid y &\\sim \\mbox{Gamma}(a+1, b+y)\n\\end{align}\n\\]",
    "crumbs": [
      "Concepts and Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Examples: continuous variables</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Heiner, Matthew, Herbert Lee, Abel Rodriguez, Raquel Prado, and Jizhou\nKang. n.d. “Bayesian Statistics.” Coursera.\n\n\nKolmogorov, Andreĭ Nikolaevich. 2018. Foundations of the Theory of\nProbability: Second English Edition. 2nd ed. Dover Publications.\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial\nwith r, JAGS, and Stan. 2nd ed. Academic Press.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. MIT Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapters/appendices/binary_variable.html",
    "href": "chapters/appendices/binary_variable.html",
    "title": "Appendix A — Binary variable",
    "section": "",
    "text": "A.1 Bernoulli distribution\nA Bernoulli distribution is used to model a random variable that can take just two discrete values, e.g., heads or tails, or failure or success. Let’s say for example that we have a fair coin. The probability that the coin lands heads up when we toss it, is denoted: \\[\n\\mbox{Pr}(X=x) = \\mbox{Pr}(X=\\mathrm{heads}) = \\theta\n\\]\nand the probability that it lands tails up is: \\[\n\\mbox{Pr}(X=x) = \\mbox{Pr}(X=\\mathrm{tails}) = 1 - \\theta\n\\]\nWe can write this in short as: \\[\n\\mbox{Pr}(X=x) = \\theta^{x}\\ (1 - \\theta)^{1-x}\n\\]\nIn general, the function that computes the probability of events which correspond to setting the random variable to each possible value is referred to as the probability mass function (pmf) (Murphy 2022): \\[\np(x) \\triangleq \\mbox{Pr}(X=x)\n\\]\nThe expected value of a Bernoulli random variable is: \\[\n\\begin{align}\n\\mathbb{E}[X] &= \\sum_{x \\in \\mathcal{X}} x p(x) \\\\\n&= (0) (1 - \\theta) + (1) (\\theta) = \\theta\n\\end{align}\n\\]\nThe variance, that is a measure of the spread of a distribution, is: \\[\n\\begin{align}\n\\mathbb{V}[X] &= \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n\\mathbb{E}[X^{2}] &= \\sum_{x \\in \\mathcal{X}} x^{2}\\ p(x) \\\\\n&= (0)^{2} (1 - \\theta) + (1)^{2} (\\theta) = \\theta \\\\\n\\mathbb{V}[X] &= \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n&= \\theta - \\theta^{2} = \\theta (1 - \\theta)\n\\end{align}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Binary variable</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/binary_variable.html#binomial-distribution",
    "href": "chapters/appendices/binary_variable.html#binomial-distribution",
    "title": "Appendix A — Binary variable",
    "section": "A.2 Binomial distribution",
    "text": "A.2 Binomial distribution\nThe generalization of the Bernoulli random variables, is the binomial distribution where we have \\(N\\) repeated trials. Suppose we flip a coin ten times, what is the probability of observing heads, heads, tails, heads, … We can model the number of heads, \\(X\\), as a function of the number of trials, \\(N\\), and the probability of success, \\(\\theta\\): \\[\np(x \\mid N, \\theta) = \\binom{N}{x} \\theta^{x} (1 - \\theta)^{1-x}\n\\]\nFurthermore, for the binomial distribution we have: \\[\n\\begin{align}\n\\mathbb{E}[X] &= n \\theta \\\\\n\\mathbb{V}[X] &= n \\theta (1 - \\theta)\n\\end{align}\n\\]\n\n\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Binary variable</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/continuous_variable.html",
    "href": "chapters/appendices/continuous_variable.html",
    "title": "Appendix B — Continuous random variable",
    "section": "",
    "text": "B.1 Uniform distribution\nWe define the cumulative distribution function (cdf): \\[\nP(x) = \\mbox{Pr}(X \\le x)\n\\]\nThe probability density function (pdf) is defined as the derivative of the cdf: \\[\np(x) \\triangleq \\frac{\\mathrm{d}}{\\mathrm{d}x}P(x)\n\\]\nFor any continuous random variable it holds that: \\[\n\\int_{-\\infty}^{\\infty} p(x)\\ \\mathrm{d}x = 1\n\\]\nand \\(p(x) \\ge 0\\).\n\\[\n\\begin{align}\nX &\\sim \\mbox{Uniform}(a, b) \\\\\np(x \\mid a, b) &= \\frac{1}{b - a}, \\quad a \\le x \\le b\n\\end{align}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Continuous random variable</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/continuous_variable.html#exponential-distribution",
    "href": "chapters/appendices/continuous_variable.html#exponential-distribution",
    "title": "Appendix B — Continuous random variable",
    "section": "B.2 Exponential distribution",
    "text": "B.2 Exponential distribution\nThe exponential distribution can be used to model events that occur at a particular rate. For example, the waiting time for the bus the come.\n\\[\n\\begin{align}\nX &\\sim \\mbox{Exp}(\\lambda) \\\\\np(x \\mid \\lambda) &= \\lambda e^{-\\lambda x}, \\quad x \\ge 0 \\\\\n\\mathbb{E}[X] &= 1 / \\lambda \\\\\n\\mathbb{V}[X] &= 1 / \\lambda^{2}\n\\end{align}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Continuous random variable</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/continuous_variable.html#normal-distribution",
    "href": "chapters/appendices/continuous_variable.html#normal-distribution",
    "title": "Appendix B — Continuous random variable",
    "section": "B.3 Normal distribution",
    "text": "B.3 Normal distribution\n\\[\n\\begin{align}\nX &\\sim \\mbox{Normal}(\\mu, \\sigma) \\\\\np(x \\mid \\mu, \\sigma) &= \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left\\{-\\frac{1}{2 \\sigma^{2}} \\left(x - \\mu\\right)^{2}\\right\\} \\\\\n\\mathbb{E}[X] &= \\mu \\\\\n\\mathbb{V}[X] &= \\sigma^{2}\n\\end{align}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Continuous random variable</span>"
    ]
  }
]